{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Dataset API\n",
    "\n",
    "Lab from https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/solutions/2_dataset_api.ipynb with personal notes nd comments.\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Learn how to use tf.data to read data from memory\n",
    "1. Learn how to use tf.data in a training loop\n",
    "1. Learn how to use tf.data to read data from disk\n",
    "1. Learn how to write production input pipelines with feature engineering (batching, shuffling, etc.)\n",
    "\n",
    "\n",
    "In this notebook, we will start by refactoring the linear regression we implemented in the previous lab so that it takes data from a`tf.data.Dataset`, and we will learn how to implement **stochastic gradient descent** with it. In this case, the original dataset will be synthetic and read by the `tf.data` API directly  from memory.\n",
    "\n",
    "In a second part, we will learn how to load a dataset with the `tf.data` API when the dataset resides on disk.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__  in this student lab notebook -- try to complete this notebook first and then review the [solution notebook](../solutions/2_dataset_api.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)\n",
    "\n",
    "if tf.version.VERSION != '2.19.0':\n",
    "    warnings.warn('Running an untested version of the Tensorflow API.')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the synthetic dataset of the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_POINTS = 10\n",
    "X = tf.constant(range(N_POINTS), dtype=tf.float32)\n",
    "Y = 2 * X + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with implementing a function that takes as input\n",
    "\n",
    "- our $X$ and $Y$ vectors of synthetic data generated by the linear function $y= 2x + 10$\n",
    "- the number of passes over the dataset we want to train on (`epochs`)\n",
    "- the size of the batches the dataset (`batch_size`)\n",
    "\n",
    "and returns a `tf.data.Dataset`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(X, Y, epochs, batch_size, is_training = False):\n",
    "    \"\"\"Builds a dataset with rules on how to iterate through it (during training).\n",
    "    \"\"\"\n",
    "\n",
    "    # Note: # the input can also be passed as: {'X': X, 'Y': Y}\n",
    "    dataset = tf.data.Dataset.from_tensor_slices( (X, Y)) \n",
    "\n",
    "    # Note 1: The last batch may not contain the exact number of elements, hence drop_remainder=True.\n",
    "    # Note 2: epochs sets a maximum to the number of iterations in the training process. If epochs \n",
    "    #   is null, we iterate through the dataset infinitely. \n",
    "    \n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    if is_training:\n",
    "        # If for trainig \n",
    "        dataset = dataset.shuffle(buffer_size=10, seed=12345)\n",
    "\n",
    "    # Most dataset input pipelines should end with a call to `prefetch`. This\n",
    "    # allows later elements to be prepared while the current element is being\n",
    "    # processed. This often improves latency and throughput, at the cost of\n",
    "    # using additional memory to store prefetched elements.\n",
    "\n",
    "    # Note: Like other `Dataset` methods, prefetch operates on the\n",
    "    # elements of the input dataset. It has no concept of examples vs. batches.\n",
    "    # `examples.prefetch(2)` will prefetch two elements (2 examples),\n",
    "    # while `examples.batch(20).prefetch(2)` will prefetch 2 elements\n",
    "    # (2 batches, of 20 examples each).\n",
    "    dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function by iterating twice over our dataset in batches of 3 datapoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [0. 1. 2.] y: [10. 12. 14.]\n",
      "x: [3. 4. 5.] y: [16. 18. 20.]\n",
      "x: [6. 7. 8.] y: [22. 24. 26.]\n",
      "x: [9. 0. 1.] y: [28. 10. 12.]\n",
      "x: [2. 3. 4.] y: [14. 16. 18.]\n",
      "x: [5. 6. 7.] y: [20. 22. 24.]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 3\n",
    "EPOCHS = 2\n",
    "dataset = create_dataset(X, Y, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    print(\"x:\", x.numpy(), \"y:\", y.numpy())\n",
    "    assert len(x) == BATCH_SIZE\n",
    "    assert len(y) == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function and the function that computes the gradients are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_mse(X, Y, w0, w1):\n",
    "    # Note: we write this as if we are deailing wiht one instance at the time \n",
    "    # (hence, X is a 1D vector, or a scalar in this example)\n",
    "    Y_hat = w0 * X + w1\n",
    "    errors = (Y_hat - Y)**2\n",
    "    return tf.reduce_mean(errors)\n",
    "\n",
    "def compute_gradients(X, Y, w0, w1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_mse(X, Y, w0, w1)\n",
    "    return tape.gradient(loss, [w0, w1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Manual) Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the traning loop, we will iterate directly on the `tf.data.Dataset` generated by our `create_dataset` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 0 - loss: 331.1055908203125, w0: 0.23999999463558197, w1: 0.4399999976158142\n",
      "\n",
      "STEP 100 - loss: 3.2297768592834473, w0: 2.55655837059021, w1: 6.674341678619385\n",
      "\n",
      "STEP 200 - loss: 0.48055925965309143, w0: 2.2146825790405273, w1: 8.717182159423828\n",
      "\n",
      "STEP 300 - loss: 0.07150308042764664, w0: 2.082810878753662, w1: 9.505172729492188\n",
      "\n",
      "STEP 400 - loss: 0.010638890787959099, w0: 2.03194260597229, w1: 9.809128761291504\n",
      "\n",
      "STEP 500 - loss: 0.001582985743880272, w0: 2.012321710586548, w1: 9.926374435424805\n",
      "\n",
      "STEP 600 - loss: 0.00023550672631245106, w0: 2.0047526359558105, w1: 9.971602439880371\n",
      "\n",
      "STEP 700 - loss: 3.50688278558664e-05, w0: 2.0018346309661865, w1: 9.989042282104492\n",
      "\n",
      "STEP 800 - loss: 5.220536877459381e-06, w0: 2.000706911087036, w1: 9.995771408081055\n",
      "\n",
      "STEP 900 - loss: 7.789132610014349e-07, w0: 2.0002737045288086, w1: 9.998367309570312\n",
      "\n",
      "STEP 1000 - loss: 1.1540369371232373e-07, w0: 2.000105381011963, w1: 9.999371528625488\n",
      "\n",
      "STEP 1100 - loss: 1.7078491509892046e-08, w0: 2.000040054321289, w1: 9.999757766723633\n",
      "\n",
      "STEP 1200 - loss: 2.318938596701514e-09, w0: 2.0000154972076416, w1: 9.99991226196289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 250\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = .02\n",
    "\n",
    "MSG = \"STEP {step} - loss: {loss}, w0: {w0}, w1: {w1}\\n\"\n",
    "\n",
    "w0 = tf.Variable(0.0)\n",
    "w1 = tf.Variable(0.0)\n",
    "\n",
    "dataset = create_dataset(X, Y, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "for step, (X_batch, Y_batch) in enumerate(dataset): \n",
    "    # compute delta\n",
    "    dw0, dw1 = compute_gradients(X_batch, Y_batch, w0, w1)\n",
    "    w0.assign_sub(dw0 * LEARNING_RATE)\n",
    "    w1.assign_sub(dw1 * LEARNING_RATE)\n",
    "    \n",
    "    #Â print(MSG.format(step=step, loss=loss, w0=w0.numpy(), w1=w1.numpy()))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        loss = loss_mse(X, Y, w0, w1)\n",
    "        print(MSG.format(step=step, loss=loss, w0=w0.numpy(), w1=w1.numpy()))\n",
    "        \n",
    "assert loss < 0.0001\n",
    "assert abs(w0 - 2) < 0.001\n",
    "assert abs(w1 - 10) < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use tf.data to read the CSV files\n",
    "\n",
    "The `tf.data` API can easily read csv files using the helper function tf.data.experimental.make_csv_dataset\n",
    "\n",
    "If you have TFRecords (which is recommended), you may use tf.data.experimental.make_batched_features_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to define \n",
    "\n",
    "- the feature names into a list `CSV_COLUMNS`\n",
    "- their default values into a list `DEFAULTS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.3</td>\n",
       "      <td>2011-01-28 20:42:59 UTC</td>\n",
       "      <td>-73.999022</td>\n",
       "      <td>40.739146</td>\n",
       "      <td>-73.990369</td>\n",
       "      <td>40.717866</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2011-06-27 04:28:06 UTC</td>\n",
       "      <td>-73.987443</td>\n",
       "      <td>40.729221</td>\n",
       "      <td>-73.979013</td>\n",
       "      <td>40.758641</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.5</td>\n",
       "      <td>2011-04-03 00:54:53 UTC</td>\n",
       "      <td>-73.982539</td>\n",
       "      <td>40.735725</td>\n",
       "      <td>-73.954797</td>\n",
       "      <td>40.778388</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.2</td>\n",
       "      <td>2009-04-10 04:11:56 UTC</td>\n",
       "      <td>-74.001945</td>\n",
       "      <td>40.740505</td>\n",
       "      <td>-73.913850</td>\n",
       "      <td>40.758559</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.5</td>\n",
       "      <td>2014-02-24 18:22:00 UTC</td>\n",
       "      <td>-73.993372</td>\n",
       "      <td>40.753382</td>\n",
       "      <td>-73.860900</td>\n",
       "      <td>40.732897</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fare_amount          pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0         11.3  2011-01-28 20:42:59 UTC        -73.999022        40.739146   \n",
       "1          7.7  2011-06-27 04:28:06 UTC        -73.987443        40.729221   \n",
       "2         10.5  2011-04-03 00:54:53 UTC        -73.982539        40.735725   \n",
       "3         16.2  2009-04-10 04:11:56 UTC        -74.001945        40.740505   \n",
       "4         33.5  2014-02-24 18:22:00 UTC        -73.993372        40.753382   \n",
       "\n",
       "   dropoff_longitude  dropoff_latitude  passenger_count  key  \n",
       "0         -73.990369         40.717866                1    0  \n",
       "1         -73.979013         40.758641                1    1  \n",
       "2         -73.954797         40.778388                1    2  \n",
       "3         -73.913850         40.758559                1    3  \n",
       "4         -73.860900         40.732897                2    4  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_DATA_FOLDER = Path('data')\n",
    "\n",
    "# The data have no headers: define the feature names here\n",
    "CSV_COLUMNS = [\n",
    "    'fare_amount',\n",
    "    'pickup_datetime',\n",
    "    'pickup_longitude',\n",
    "    'pickup_latitude',\n",
    "    'dropoff_longitude',\n",
    "    'dropoff_latitude',\n",
    "    'passenger_count',\n",
    "    'key'\n",
    "]\n",
    "# Target variable\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "\n",
    "# Defining the default values into a list `DEFAULTS`\n",
    "DEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]\n",
    "\n",
    "df = pd.read_csv( 'data/taxi-train.csv', names = CSV_COLUMNS, nrows=10)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=OrderedDict([('fare_amount', TensorSpec(shape=(3,), dtype=tf.float32, name=None)), ('pickup_datetime', TensorSpec(shape=(3,), dtype=tf.string, name=None)), ('pickup_longitude', TensorSpec(shape=(3,), dtype=tf.float32, name=None)), ('pickup_latitude', TensorSpec(shape=(3,), dtype=tf.float32, name=None)), ('dropoff_longitude', TensorSpec(shape=(3,), dtype=tf.float32, name=None)), ('dropoff_latitude', TensorSpec(shape=(3,), dtype=tf.float32, name=None)), ('passenger_count', TensorSpec(shape=(3,), dtype=tf.float32, name=None)), ('key', TensorSpec(shape=(3,), dtype=tf.string, name=None))])>\n"
     ]
    }
   ],
   "source": [
    "# Create a tensorflow dataset for exploration only (no transformations)\n",
    "def create_dataset_without_transformations(pattern: str, batch_size=10):\n",
    "    \"\"\"Load csv data from a path (which can be a pattern, i.e. include the `*` symbol) and return a\n",
    "    tf.data.Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.data.experimental.make_csv_dataset(\n",
    "        # Note that this is, by default\n",
    "        pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS)\n",
    "\n",
    "tempds = create_dataset_without_transformations( 'data/taxi-*.csv', batch_size=3)\n",
    "print(tempds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a **prefetched dataset, where each element is an `OrderedDict` whose keys are the feature names and whose values are tensors of shape `(batch_size,)` (i.e. vectors).**\n",
    "\n",
    "Let's iterate over the two first element of this dataset using `dataset.take(2)` and let's convert them ordinary Python dictionary with numpy array as values for more readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropoff_latitude': array([40.744682, 40.75461 , 40.735806], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.98476 , -73.9798  , -73.989624], dtype=float32),\n",
      " 'fare_amount': array([15. ,  4. ,  5.3], dtype=float32),\n",
      " 'key': array([b'1714', b'236', b'437'], dtype=object),\n",
      " 'passenger_count': array([1., 1., 1.], dtype=float32),\n",
      " 'pickup_datetime': array([b'2014-02-24 18:22:00 UTC', b'2014-10-06 15:16:00 UTC',\n",
      "       b'2009-11-13 07:20:07 UTC'], dtype=object),\n",
      " 'pickup_latitude': array([40.778023, 40.761875, 40.745148], dtype=float32),\n",
      " 'pickup_longitude': array([-73.94582 , -73.97502 , -73.998215], dtype=float32)}\n",
      "\n",
      "\n",
      "{'dropoff_latitude': array([40.758404, 40.787586, 40.7581  ], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.98778, -73.97701, -73.97735], dtype=float32),\n",
      " 'fare_amount': array([5.5, 7.5, 6. ], dtype=float32),\n",
      " 'key': array([b'1904', b'1135', b'57'], dtype=object),\n",
      " 'passenger_count': array([1., 1., 1.], dtype=float32),\n",
      " 'pickup_datetime': array([b'2013-05-28 12:58:30 UTC', b'2009-02-04 18:18:59 UTC',\n",
      "       b'2013-07-18 16:02:19 UTC'], dtype=object),\n",
      " 'pickup_latitude': array([40.762066, 40.806828, 40.762016], dtype=float32),\n",
      " 'pickup_longitude': array([-74.00095, -73.965  , -73.96837], dtype=float32)}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data in tempds.take(2):\n",
    "    pprint({k: v.numpy() for k, v in data.items()})\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we want to do two things:\n",
    "1. Do something such that, when we iterate through the dataset for custom training, we can separate features from label (as in previous example).\n",
    "2. Apply some transformation to the features (e.g. drop some features).\n",
    "\n",
    "This can be achieved writing a custom function that does both (1) and (2), and pass it to the `tf.data.Dataset` object through the `map` method, that will apply the transformation to each element of the dataset as it iterates. If used with the `prefetch` method, this can lead to a very efficient implementation, where we transform the data while we train the model.\n",
    "\n",
    "\n",
    "\n",
    "Let's first implement a function that takes as input a row (represented as an `OrderedDict` in our `tf.data.Dataset` as above) and then returns a tuple with two elements:\n",
    "\n",
    "* The first element being the same `OrderedDict` with the label dropped\n",
    "* The second element being the label itself (`fare_amount`)\n",
    "\n",
    "Note that we will need to also remove the `key` and `pickup_datetime` column, which we won't use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNWANTED_COLS = ['pickup_datetime', 'key']\n",
    "from collections import OrderedDict\n",
    "\n",
    "def features_and_labels(row_data: OrderedDict):\n",
    "    \"\"\"This function applies a transformation to each row of the dataset. As we saw that each row is\n",
    "    treated as an OrderedDict, we need to write the function to operate on it.\n",
    "\n",
    "    Returns:\n",
    "        OrderedDict: containing the features we want to use.\n",
    "        label_value: the value needed for training the model. Why this is not an OrderedDict?\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # if we wanted to preserve row_data, here we would need to make a copy of this dictionary. However, \n",
    "    # in the context of batch training and iteration of the dataset losing info from row_data is \n",
    "    # irrelevant. We are, in fact, continuously readying new data (i.e. a new row_data object), dropping\n",
    "    # some keys and moving on to the next one.\n",
    "    features = row_data\n",
    "    \n",
    "    # remove label...\n",
    "    label_value = features.pop(LABEL_COLUMN)\n",
    "    \n",
    "    # ... and unwanted features\n",
    "    for key_to_remove in UNWANTED_COLS:\n",
    "        _ = features.pop(key_to_remove)\n",
    "    \n",
    "    return features, label_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over 2 examples from our `tempds` dataset and apply our `feature_and_labels`\n",
    "function to each of the examples to make sure it's working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('pickup_longitude',\n",
      "              <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-73.99418, -73.96282, -73.9576 ], dtype=float32)>),\n",
      "             ('pickup_latitude',\n",
      "              <tf.Tensor: shape=(3,), dtype=float32, numpy=array([40.73548 , 40.76987 , 40.761368], dtype=float32)>),\n",
      "             ('dropoff_longitude',\n",
      "              <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-73.986336, -73.9871  , -73.954254], dtype=float32)>),\n",
      "             ('dropoff_latitude',\n",
      "              <tf.Tensor: shape=(3,), dtype=float32, numpy=array([40.729626, 40.728333, 40.77478 ], dtype=float32)>),\n",
      "             ('passenger_count',\n",
      "              <tf.Tensor: shape=(3,), dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>)])\n",
      "tf.Tensor([ 5.  13.5  6.3], shape=(3,), dtype=float32) \n",
      "\n",
      "OrderedDict([('pickup_longitude',\n",
      "              <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-73.9828 , -73.98509, -73.96314], dtype=float32)>),\n",
      "             ('pickup_latitude',\n",
      "              <tf.Tensor: shape=(3,), dtype=float32, numpy=array([40.73917 , 40.747623, 40.769463], dtype=float32)>),\n",
      "             ('dropoff_longitude',\n",
      "              <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-73.97494 , -73.97375 , -73.981636], dtype=float32)>),\n",
      "             ('dropoff_latitude',\n",
      "              <tf.Tensor: shape=(3,), dtype=float32, numpy=array([40.750145, 40.736874, 40.77862 ], dtype=float32)>),\n",
      "             ('passenger_count',\n",
      "              <tf.Tensor: shape=(3,), dtype=float32, numpy=array([1., 1., 2.], dtype=float32)>)])\n",
      "tf.Tensor([6.5 6.5 8.1], shape=(3,), dtype=float32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 3\n",
    "for row_data in tempds.take(2):\n",
    "    features, label = features_and_labels(row_data)\n",
    "    pprint(features)\n",
    "    print(label, \"\\n\")\n",
    "    # assert UNWANTED_COLS[0] not in features.keys()\n",
    "    # assert UNWANTED_COLS[1] not in features.keys()\n",
    "    assert label.shape == [BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting things together: reading from disk and and transform while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorflow dataset for exploration only (no transformations)\n",
    "def create_dataset_with_transformations(pattern: str, batch_size=10, mode='eval'):\n",
    "    \"\"\"Load csv data from a path (which can be a pattern, i.e. include the `*` symbol) and return a\n",
    "    tf.data.Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS)\n",
    "\n",
    "    dataset = dataset.map(features_and_labels).cache()\n",
    "\n",
    "    if mode == 'train':\n",
    "        dataset = dataset.shuffle(12345).repeat()\n",
    "\n",
    "    # take advantage of multi-threading; 1=AUTOTUNE\n",
    "    #Â tf.data.AUTOTUNE\n",
    "    dataset = dataset.prefetch(1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that our batches are of the right size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropoff_latitude': array([40.643425, 40.75328 , 40.75985 ], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.78996, -73.97293, -73.98227], dtype=float32),\n",
      " 'passenger_count': array([1., 1., 1.], dtype=float32),\n",
      " 'pickup_latitude': array([40.761078, 40.758465, 40.732746], dtype=float32),\n",
      " 'pickup_longitude': array([-73.97889, -73.98506, -73.98142], dtype=float32)}\n",
      "Y = [np.float32(52.0), np.float32(5.5), np.float32(10.5)]\n",
      "\n",
      "{'dropoff_latitude': array([40.763275, 40.741783, 40.78058 ], dtype=float32),\n",
      " 'dropoff_longitude': array([-73.99644, -73.99738, -73.94664], dtype=float32),\n",
      " 'passenger_count': array([3., 1., 5.], dtype=float32),\n",
      " 'pickup_latitude': array([40.74281 , 40.751762, 40.75532 ], dtype=float32),\n",
      " 'pickup_longitude': array([-73.98438 , -73.99364 , -73.983086], dtype=float32)}\n",
      "Y = [np.float32(8.9), np.float32(4.9), np.float32(12.1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 3\n",
    "\n",
    "tempds = create_dataset_with_transformations( 'data/taxi-train*.csv', batch_size=BATCH_SIZE)\n",
    "\n",
    "for X_batch, Y_batch in tempds.take(2):\n",
    "    pprint({k: v.numpy() for k, v in X_batch.items()})\n",
    "    print(f'Y = {list(Y_batch.numpy())}', end=\"\\n\\n\")\n",
    "    assert len(Y_batch) == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO SEE HOW TO USE THIS FOR TRAINING, GO TO: https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/solutions/1_training_at_scale_vertex.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling\n",
    "\n",
    "When training a deep learning model in batches over multiple workers, it is helpful if we shuffle the data. That way, different workers will be working on different parts of the input file at the same time, and so averaging gradients across workers will help. Also, during training, we will need to read the data indefinitely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refactor our `create_dataset` function so that it shuffles the data, when the dataset is used for training.\n",
    "\n",
    "We will introduce an additional argument `mode` to our function to allow the function body to distinguish the case\n",
    "when it needs to shuffle the data (`mode == 'train'`) from when it shouldn't (`mode == 'eval'`).\n",
    "\n",
    "Also, before returning we will want to prefetch 1 data point ahead of time (`dataset.prefetch(1)`) to speed-up training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lab Task #4c:** The last step of our `tf.data` dataset will specify shuffling and repeating of our dataset pipeline. Complete the code below to add these three steps to the Dataset pipeline\n",
    "1. follow the `.map(...)` operation which extracts features and labels with a call to `.cache()` the result.\n",
    "2. during training, use `.shuffle(...)` and `.repeat()` to shuffle batches and repeat the dataset\n",
    "3. use `.prefetch(...)` to take advantage of multi-threading and speedup training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that our function works well in both modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempds = create_dataset('toy-data/taxi-train*', 2, 'train')\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempds = create_dataset('toy-data/taxi-train*', 2, 'eval')\n",
    "print(list(tempds.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we will build the model using this input pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
