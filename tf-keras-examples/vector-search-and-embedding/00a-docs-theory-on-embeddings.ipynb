{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55bc8bfa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Index\n",
    "\n",
    "- [Encoding Data with Embeddings](#encoding-data-with-embeddings)\n",
    "    - [The Challenge of Text Representation](#the-challenge-of-text-representation)\n",
    "    - [Basic Vectorization: One-Hot Encoding](#basic-vectorization-one-hot-encoding)\n",
    "    - [Text Embeddings (Word Embeddings): Dense Representations](#text-embeddings-word-embeddings-dense-representations)\n",
    "    - [Multimodal Embeddings](#multimodal-embeddings)\n",
    "\n",
    "- [Generating Embeddings for Variable-Length Data](#generating-embeddings-for-variable-length-data)\n",
    "    - [Embeddings for Sentences / Variable-Length Text Sequences](#1-embeddings-for-sentences--variable-length-text-sequences)\n",
    "    - [Embeddings for Images / Variable-Resolution Data](#2-embeddings-for-images--variable-resolution-data)\n",
    "\n",
    "- [Focus on Sentence Embedding using Tranformer Models](#focus-on-sentence-embedding-using-tranformer-models)\n",
    "    - [Why Vanilla BERT Fails at Sentence Embeddings](#why-vanilla-bert-fails-at-sentence-embeddings)\n",
    "    - [Sentence-BERT's Solution: Siamese and Triplet Networks](#sentence-berts-solution-siamese-and-triplet-networks)\n",
    "    - [The Key Difference: Fine-tuning for Similarity](#the-key-difference-fine-tuning-for-similarity)\n",
    "    - [Why this is a Game Changer](#why-this-is-a-game-changer)\n",
    "\n",
    "- [Indexing and Search](#indexing-and-search)\n",
    "    - [Measuring Vector Distance (Similarity Metrics)](#1-measuring-vector-distance-similarity-metrics)\n",
    "    - [Fast and Scalable Vector Search](#2-fast-and-scalable-vector-search)\n",
    "    - [Vertex AI Vector Search (formerly Matching Engine)](#vertex-ai-vector-search-formerly-matching-engine)\n",
    "\n",
    "- [TF-IDF (Term Frequency-Inverse Document Frequency)](#tf-idf-term-frequency-inverse-document-frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae8bbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f5e88e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f262b3c7",
   "metadata": {},
   "source": [
    "# Encoding Data with Embeddings\n",
    "\n",
    "## Introduction to Embeddings\n",
    "\n",
    "Embeddings are a fundamental technology for **encoding data into a representation that captures semantic meaning**. This process addresses the challenge of transforming complex data (like text, images, audio, video, or code) into a numerical format that retains its underlying sense.\n",
    "\n",
    "While embeddings can be generated for multimodal data, this documentation focuses on **text embeddings** as a primary example.\n",
    "\n",
    "## The Challenge of Text Representation\n",
    "\n",
    "The core problem is: **How do you represent text numerically while retaining its meaning?** This can be broken down into two sub-problems:\n",
    "\n",
    "1.  **Semantic Relationship:** How do you convert text into numbers that reflect semantic relationships between words, indicating their similarity in meaning?\n",
    "2.  **Machine Learning Input:** How do you transform text into numbers that can be efficiently processed by machine learning models, typically requiring relatively dense vectors to avoid overfitting?\n",
    "\n",
    "## Basic Vectorization: One-Hot Encoding\n",
    "\n",
    "One of the most intuitive, but basic, techniques for text representation is **one-hot encoding**.\n",
    "\n",
    "1.  **Tokenization & Preprocessing:** A sentence is divided into smaller units (tokens, often words) and preprocessed (e.g., stemming to get root words).\n",
    "2.  **Vocabulary:** A vocabulary of all unique words in the dataset is created. This can easily contain tens of thousands of words.\n",
    "3.  **Vector Creation:** \n",
    "\n",
    "While intuitive and easy to implement, it has the following **disadvantages**:\n",
    "    1.  **No Semantic Relationship:** One-hot encoding does not convey any relationship between words. \"Dog\" and \"cat\" are as \"far apart\" as \"dog\" and \"apple\" in this representation, even though \"dog\" and \"cat\" are semantically much closer.\n",
    "    2.  **High-Dimensional and Sparse:** The resulting vectors are very high-dimensional (equal to the vocabulary size) and extremely sparse (mostly zeros).\n",
    "        * A vocabulary of 10,000 words leads to 10,000-dimensional vectors, with 99.99% zeros.\n",
    "        * This **sparse embedding** can lead to computational inefficiency and model overfitting in machine learning tasks.\n",
    "\n",
    "## Text Embeddings (Word Embeddings): Dense Representations\n",
    "\n",
    "To overcome the limitations of sparse representations, **text embeddings** (also known as word embeddings or dense embeddings) were developed. Text embeddings leverage this idea by representing each word as a point in a multi-dimensional vector space.\n",
    "\n",
    "### Key Characteristics:\n",
    "\n",
    "  * **Semantic Similarity:** The distance between words in this vector space indicates their semantic similarity.\n",
    "      * *Example:* \"Queen\" and \"king\" are close to each other, but \"queen\" and \"apple\" are far apart.\n",
    "  * **Analogies:** Embeddings can capture analogies between words.\n",
    "      * *Example:* The vector difference between \"king\" and \"man\" is similar to the difference between \"queen\" and \"woman.\" This allows for operations like: `vector(\"king\") - vector(\"man\") + vector(\"woman\") = vector(\"queen\")`.\n",
    "  * **Low-Dimensional & Dense:** Unlike one-hot encoding, text embeddings use significantly fewer dimensions (e.g., tens to thousands, instead of tens of thousands). Each dimension typically contains a non-zero value, making them **dense embeddings**.\n",
    "  * **Distributed Representation:** The meaning of a word is \"distributed\" across these dimensions, with each dimension capturing a specific feature or aspect of the word.\n",
    "\n",
    "### How are Text Embeddings Developed?\n",
    "\n",
    "Instead of manually assigning values, text embeddings are **learned by neural networks**. These networks are trained on vast amounts of text data to identify patterns and relationships between words. Popular algorithms and models used for this include:\n",
    "\n",
    "  * **Word2Vec** (by Google)\n",
    "  * **GloVe** (Global Vectors for Word Representation, by Stanford)\n",
    "  * **FastText** (by Facebook/Meta)\n",
    "\n",
    "Training these models from scratch requires substantial computational resources. Fortunately, **pre-trained embedding models** are widely available via APIs, simplifying their use.\n",
    "\n",
    "### Using Embedding APIs:\n",
    "\n",
    "Using embedding APIs is typically straightforward:\n",
    "\n",
    "1.  **Specify Model:** Choose a pre-trained embedding model (e.g., `text-embedding-gecko` or `multimodal-embedding` from Google).\n",
    "2.  **Define Input:** Provide the text you want to embed.\n",
    "3.  **Get Embeddings:** Call the API to receive the dense vector representation of your text.\n",
    "\n",
    "### Multimodal Embeddings\n",
    "\n",
    "The same approach used for text can be applied to other media types, such as images. This allows for **multimodal embeddings**, where different types of data (e.g., the text \"dog\" and an image of a dog) are represented in the same vector space, located near each other due to their semantic similarity.\n",
    "\n",
    "Embeddings are a powerful concept that allows computers to \"understand\" and process the semantic meaning of various data types, forming a critical component of modern AI and machine learning applications.\n",
    "\n",
    "-----\n",
    "\n",
    "## Further Reading & Resources\n",
    "\n",
    "To dive deeper into Embeddings, text representation, and the models used to create them:\n",
    "\n",
    "### Embeddings & Models:\n",
    "\n",
    "  * **Hugging Face Hub (Models):**\n",
    "      * A vast repository of pre-trained models, including many for generating embeddings (e.g., Sentence-BERT, OpenAI's embedding models).\n",
    "      * [Website: huggingface.co/models](https://huggingface.co/models)\n",
    "  * **OpenAI Embeddings:**\n",
    "      * OpenAI offers powerful embedding models accessible via API, widely used for various semantic tasks.\n",
    "      * [OpenAI Embeddings Documentation](https://platform.openai.com/docs/guides/embeddings)\n",
    "  * **Google's Universal Sentence Encoder:**\n",
    "      * A model for encoding text into high-dimensional vectors.\n",
    "      * [TensorFlow Hub: Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4)\n",
    "\n",
    "### Core Concepts & Algorithms:\n",
    "\n",
    "  * **Word2Vec Explained:**\n",
    "      * A classic paper and many tutorials exist. Search for \"Word2Vec tutorial\" or \"Gensim Word2Vec\" for practical examples.\n",
    "      * [Original Paper: Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "  * **GloVe: Global Vectors for Word Representation:**\n",
    "      * Another foundational method for learning word embeddings.\n",
    "      * [Original Paper: GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "  * **FastText:**\n",
    "      * An extension to Word2Vec that considers subword information, useful for rare words.\n",
    "      * [FastText Website](https://fasttext.cc/)\n",
    "  * **Sparse vs. Dense Embeddings:**\n",
    "      * Understanding the differences is crucial for performance and quality in search systems.\n",
    "      * [Article: Sparse vs. Dense Vectors for Search](https://www.google.com/search?q=https://www.pinecone.io/learn/sparse-dense-vectors-for-search/)\n",
    "\n",
    "### Practical Implementation & APIs:\n",
    "\n",
    "  * **OpenAI Embeddings:**\n",
    "      * Learn how to use OpenAI's powerful text embedding models via their API.\n",
    "      * [OpenAI Embeddings Documentation](https://platform.openai.com/docs/guides/embeddings)\n",
    "  * **Google Cloud Embeddings:**\n",
    "      * Explore Google's offerings for generating text and multimodal embeddings.\n",
    "      * [Google Cloud - Generate text embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings)\n",
    "  * **Hugging Face Transformers Library:**\n",
    "      * A comprehensive library for various NLP tasks, including using and fine-tuning embedding models.\n",
    "      * [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
    "  * **Sentence-BERT (SBERT):**\n",
    "      * A popular method for generating sentence-level embeddings, often used with vector search.\n",
    "      * [Sentence-BERT GitHub](https://github.com/UKPLab/sentence-transformers)\n",
    "\n",
    "### Natural Language Processing (NLP) Courses:\n",
    "\n",
    "  * **Natural Language Processing on Google Cloud (Coursera/Google Cloud Skills Boost):**\n",
    "      * This course often includes modules on text representation, embeddings, and practical applications. Search for the most up-to-date version.\n",
    "      * [Search Google Cloud Skills Boost for \"Natural Language Processing on Google Cloud\"](https://www.google.com/search?q=Natural+Language+Processing+on+Google+Cloud+skills+boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c90ee",
   "metadata": {},
   "source": [
    "# Generating Embeddings for Variable-Length Data\n",
    "\n",
    "The challenge of variable input size is central to processing sentences, images, audio, or any data where the \"length\" or \"resolution\" isn't fixed. Autoencoders with fixed input/output layers, as you described, are not directly suitable for these tasks. Instead, more sophisticated architectures are used.\n",
    "\n",
    "## 1\\. Embeddings for Sentences / Variable-Length Text Sequences\n",
    "\n",
    "For sentences and longer text sequences, the primary goal is to get a single, fixed-size vector representation for the entire sequence that captures its overall meaning. Autoencoders are less common for this specific task because they are designed for reconstruction, whereas sentence embedding often focuses on meaning representation directly.\n",
    "\n",
    "Here's how it's typically done:\n",
    "\n",
    "### a) Recurrent Neural Networks (RNNs) and their Variants (LSTM, GRU)\n",
    "\n",
    "  * **Concept:** RNNs are specifically designed to process sequential data. They maintain a \"hidden state\" that acts as a memory, updating it as each word in the sequence is processed.\n",
    "  * **How it Works:**\n",
    "    1.  Each word in the sentence is first converted into a **word embedding** (e.g., using Word2Vec, GloVe, or a pre-trained embedding layer).\n",
    "    2.  These word embeddings are fed sequentially into the RNN.\n",
    "    3.  The final hidden state of the RNN (or a pooled/aggregated version of all hidden states) can be used as the **sentence embedding**. This final state effectively summarizes the information learned from the entire sequence into a fixed-size vector.\n",
    "  * **Variable Length Handling:** RNNs naturally handle variable-length sequences because they process one word at a time, updating their internal state, regardless of how many words are in the sequence. The output (the sentence embedding) always has the same fixed dimension.\n",
    "  * **Drawbacks:** Traditional RNNs struggle with long-range dependencies (the \"vanishing/exploding gradient\" problem). LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are improvements that mitigate this.\n",
    "\n",
    "### b) Convolutional Neural Networks (CNNs) for Text\n",
    "\n",
    "  * **Concept:** While famously used for images, CNNs can also be applied to text. They use filters (kernels) to identify local patterns (like n-grams) within a sequence.\n",
    "  * **How it Works:**\n",
    "    1.  Word embeddings are stacked to form a \"matrix\" representing the sentence.\n",
    "    2.  Convolutional filters slide over this matrix, detecting patterns (e.g., phrases).\n",
    "    3.  A pooling layer (e.g., max-pooling) is then applied across the results of these filters to select the most salient features.\n",
    "    4.  The output of the pooling layer, or further layers, can then form the fixed-size sentence embedding.\n",
    "  * **Variable Length Handling:** The pooling step (especially global max-pooling) is key here. It reduces the variable-length output of the convolutions to a fixed-size vector, regardless of the input sentence length.\n",
    "\n",
    "### c) Transformer Models (e.g., BERT, Sentence-BERT)\n",
    "\n",
    "  * **Concept:** Transformers are the state-of-the-art for many NLP tasks, heavily relying on the \"attention mechanism.\" They process all words in a sequence simultaneously, but the attention mechanism allows them to weigh the importance of different words relative to each other.\n",
    "  * **How it Works:**\n",
    "    1.  Input words are first converted into embeddings.\n",
    "    2.  These embeddings, along with positional encodings (to retain word order), are passed through multiple \"transformer blocks.\"\n",
    "    3.  The attention mechanism learns to focus on different parts of the input sentence when generating representations for each word.\n",
    "    4.  To get a sentence embedding, typically:\n",
    "          * The embedding of a special `[CLS]` (classification) token at the beginning of the sequence is used (as in BERT).\n",
    "          * Or, the embeddings of all tokens are pooled (e.g., mean-pooled or max-pooled) to create a single vector (as in Sentence-BERT).\n",
    "  * **Variable Length Handling:** Transformers use **padding** to make all input sequences the same length for batch processing. However, a \"mask\" is used to tell the attention mechanism to ignore the padded tokens. The final output (e.g., `[CLS]` token embedding or pooled embeddings) is always a fixed-size vector, irrespective of the original sequence length.\n",
    "  * **Why they're powerful:** They excel at capturing long-range dependencies and complex semantic relationships due to their parallel processing and sophisticated attention.\n",
    "\n",
    "  * **Note: Vanilla BERT does not work well**. If you take a BERT model and you use it to geenrate sentence embedding with the methods described above, it will produce poor embeddings. These models required an extra layer and supervised training to be *specialised* for sentnce embedding. See [Focus on Sentence Embedding using Tranformer Models](#focus-on-sentence-embedding-using-tranformer-models) for more info.\n",
    "\n",
    "### Relevant Links for Sentence Embeddings:\n",
    "\n",
    "  * **Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks:**\n",
    "      * A seminal paper and library for high-quality sentence embeddings.\n",
    "      * [Paper: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)\n",
    "      * [GitHub: sentence-transformers](https://github.com/UKPLab/sentence-transformers)\n",
    "  * **What are embeddings? (Google Developers):**\n",
    "      * Good general overview of embeddings, including for text.\n",
    "      * [Link: What are embeddings?](https://www.google.com/search?q=https://developers.google.com/machine-learning/glossary/embedding)\n",
    "  * **The Illustrated Transformer:**\n",
    "      * A fantastic visual explanation of how transformer models work.\n",
    "      * [Link: The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "## 2\\. Embeddings for Images / Variable-Resolution Data\n",
    "\n",
    "Images (and other variable-resolution data like medical scans or videos) also present the challenge of non-uniform input sizes.\n",
    "\n",
    "### a) Convolutional Neural Networks (CNNs)\n",
    "\n",
    "  * **Concept:** CNNs are the workhorse of modern computer vision. They learn hierarchical features from images through convolutional layers that detect patterns (edges, textures, shapes) at various scales.\n",
    "  * **How it Works:**\n",
    "    1.  An image is fed into a stack of convolutional layers, interleaved with pooling layers (e.g., max-pooling).\n",
    "    2.  Convolutional layers apply filters across local regions of the image, producing feature maps.\n",
    "    3.  Pooling layers downsample these feature maps, reducing their spatial dimensions and making the network more robust to small variations in the input.\n",
    "    4.  After several convolutional and pooling layers, the output is typically flattened into a 1D vector.\n",
    "    5.  This flattened vector is then fed into one or more fully connected (dense) layers. The output of one of these dense layers (often before the final classification layer) serves as the **image embedding**.\n",
    "  * **Variable Resolution Handling:**\n",
    "      * **Resizing/Cropping:** Often, images are **resized** or **cropped** to a fixed input dimension (e.g., 224x224 pixels for ImageNet-trained models) *before* being fed into the CNN. This is the simplest and most common approach, though it can lose information or introduce distortion.\n",
    "      * **Global Pooling:** For truly variable input sizes, the final pooling layer can be a **global average pooling** or **global max pooling** layer. This layer takes the entire feature map from the preceding convolutional layer and reduces it to a single value per feature channel, resulting in a fixed-size vector regardless of the input image's original dimensions (after convolutions).\n",
    "      * **Adaptive Pooling:** Some architectures use adaptive pooling layers that can automatically adjust their kernel size to produce a fixed-size output, regardless of the input feature map size.\n",
    "\n",
    "### b) Vision Transformers (ViT)\n",
    "\n",
    "  * **Concept:** Adapting the successful Transformer architecture from NLP to computer vision.\n",
    "  * **How it Works:**\n",
    "    1.  An image is divided into a grid of fixed-size **patches** (e.g., 16x16 pixels).\n",
    "    2.  Each patch is then flattened into a 1D vector and linearly projected into a specific dimension.\n",
    "    3.  Positional embeddings are added to these patch embeddings (to retain spatial information).\n",
    "    4.  These sequence of patch embeddings are then fed into a standard Transformer encoder, similar to how text tokens are handled.\n",
    "    5.  The output embedding of a special `[CLS]` token (similar to BERT) or the pooled embeddings of all patches can serve as the **image embedding**.\n",
    "  * **Variable Resolution Handling:**\n",
    "      * Similar to text transformers, the input image is effectively converted into a sequence of fixed-size patches. While the *number* of patches can vary with image resolution, the *size* of each patch is fixed.\n",
    "      * Models are often trained on fixed-resolution images initially. To handle different resolutions, images are either resized or the positional embeddings are interpolated. The output embedding itself is always fixed-size.\n",
    "\n",
    "### Relevant Links for Image Embeddings:\n",
    "\n",
    "  * **ImageNet and CNNs:**\n",
    "      * Understanding ImageNet is key to understanding pre-trained CNNs for image embeddings.\n",
    "      * [Article: A Brief Introduction to ImageNet](https://www.google.com/search?q=https://towardsdatascience.com/a-brief-introduction-to-imagenet-c-a7c37f39b69b)\n",
    "  * **Introduction to CNNs for Image Recognition (Keras):**\n",
    "      * A good practical overview.\n",
    "      * [Keras Guide: A crash course on CNNs](https://keras.io/api/layers/convolution_layers/convolution2d/)\n",
    "  * **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Vision Transformers):**\n",
    "      * The original paper for ViT.\n",
    "      * [Paper: An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)\n",
    "  * **CLIP (Contrastive Language-Image Pre-training):**\n",
    "      * A model that learns highly effective multimodal embeddings by training on image-text pairs, allowing you to get embeddings for both images and text in the same space.\n",
    "      * [OpenAI Blog: CLIP](https://openai.com/research/clip)\n",
    "\n",
    "-----\n",
    "\n",
    "In summary, the key to handling variable-length or variable-resolution inputs for embeddings lies in using architectures like **RNNs, CNNs, and Transformers** that incorporate mechanisms (like recurrent states, pooling layers, or attention with padding/patching) to distill the variable input into a fixed-size, meaningful vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d413ed",
   "metadata": {},
   "source": [
    "# Focus on Sentence Embedding using Tranformer Models\n",
    "\n",
    "## Why Vanilla BERT Fails at Sentence Embeddings\n",
    "\n",
    "**ref**: [Paper: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084).\n",
    "\n",
    "A straightforward application of pre-trained BERT for sentence embeddings (e.g., using the `[CLS]` token's final hidden state or averaging all token embeddings) yields surprisingly **poor quality embeddings for semantic similarity tasks**.\n",
    "\n",
    "Here's why:\n",
    "\n",
    "1.  **BERT's Original Training Objective:**\n",
    "    * **Masked Language Modeling (MLM):** BERT is trained to predict masked words in a sentence. This forces it to understand local context and word relationships *within a single sentence*.\n",
    "    * **Next Sentence Prediction (NSP):** BERT is also trained to predict if two sentences follow each other. This helps it understand sentence-level relationships, but it's a binary classification task, not a continuous similarity task.\n",
    "    * **No Explicit Similarity Objective:** Critically, BERT's training **does not explicitly teach it to produce sentence embeddings that are close in vector space for semantically similar sentences.** It wasn't optimized for this. Its embeddings are good for contextualizing individual words for downstream tasks (like classification or Q&A), but not for direct vector comparisons of whole sentences.\n",
    "\n",
    "2.  **The `[CLS]` Token's Role:**\n",
    "    * In vanilla BERT, the `[CLS]` token's final hidden state is primarily designed for **sequence-level classification tasks**. When you fine-tune BERT for sentiment analysis or spam detection, you typically add a linear layer on top of the `[CLS]` token's embedding to make the classification. During this fine-tuning, the `[CLS]` token learns to condense information relevant to that *specific classification task*, not necessarily general semantic meaning for similarity comparisons.\n",
    "    * Without specific fine-tuning for similarity, the `[CLS]` token's embedding from the base BERT model is essentially a \"general purpose\" representation that hasn't been optimized to cluster similar sentences together in vector space.\n",
    "\n",
    "3.  **Averaging Token Embeddings:**\n",
    "    * Averaging all token embeddings (after passing them through BERT) helps, but it still often falls short. While each token embedding is contextually rich, simply averaging them can lose the nuanced, overall semantic meaning of the sentence. It's like averaging all the words in a paragraph; you get a vague idea, but not a concise summary designed for comparison.\n",
    "\n",
    "4.  **Computational Cost for Similarity Search:**\n",
    "    * If you wanted to find the most similar sentence to a query using vanilla BERT, you'd have to pass *each candidate sentence* through the BERT model separately to get its `[CLS]` token embedding, and then compute the similarity. This is extremely inefficient. For a database of millions of sentences, it would mean millions of BERT passes per query, which is computationally prohibitive.\n",
    "\n",
    "---\n",
    "\n",
    "## Sentence-BERT's Solution: Siamese and Triplet Networks\n",
    "\n",
    "SBERT's main innovation is to **fine-tune BERT (or other pre-trained transformer models) specifically for semantic similarity tasks** using a **siamese** or **triplet network architecture**.\n",
    "\n",
    "### The Core Idea: Siamese/Triplet Architectures\n",
    "\n",
    "Instead of just one BERT model, SBERT uses two (siamese) or three (triplet) identical BERT models that share the same weights.\n",
    "\n",
    "1.  **Siamese Network:**\n",
    "    * Takes **two input sentences** (e.g., a pair `(sentence_A, sentence_B)`).\n",
    "    * Each sentence is passed through an **identical BERT model**.\n",
    "    * After the BERT output, a **pooling operation** is applied to generate a fixed-size sentence embedding for each sentence (e.g., mean-pooling over all token embeddings is often used, or the `[CLS]` token's embedding, but *after* fine-tuning).\n",
    "    * These two sentence embeddings are then used to calculate a **similarity score** (e.g., cosine similarity, Euclidean distance) or are fed into a classification head to predict if they are similar/dissimilar.\n",
    "    * **Training Objective:** The network is trained with a loss function that encourages:\n",
    "        * **Similar sentence pairs** to have embeddings that are **close together** in vector space.\n",
    "        * **Dissimilar sentence pairs** to have embeddings that are **far apart**.\n",
    "\n",
    "2.  **Triplet Network:**\n",
    "    * Takes **three input sentences**: an **anchor** sentence, a **positive** sentence (semantically similar to the anchor), and a **negative** sentence (semantically dissimilar to the anchor).\n",
    "    * Each sentence is passed through an **identical BERT model**.\n",
    "    * **Training Objective:** The network is trained using a **triplet loss** function. This loss aims to ensure that the distance between the anchor and the positive embedding is *smaller* than the distance between the anchor and the negative embedding by at least some margin. This directly optimizes for creating an embedding space where similar items are clustered and dissimilar items are separated.\n",
    "\n",
    "### What SBERT Does (Specifically in the Paper):\n",
    "\n",
    "The SBERT paper explores various fine-tuning objectives, including:\n",
    "\n",
    "  * **Classification Objective:** Taking two sentences, passing them through BERT, generating embeddings, concatenating the embeddings (or their difference, or a similarity score), and then using a softmax classifier to predict if they are semantically related (e.g., entailment, contradiction, neutral). This forces the embeddings to contain information relevant for distinguishing relationships.\n",
    "  * **Regression Objective:** Taking two sentences, passing them through BERT, generating embeddings, computing cosine similarity between them, and then training to predict a target similarity score (e.g., from a dataset like STS Benchmark).\n",
    "  * **Triplet Objective:** As described above, using anchor, positive, and negative examples to explicitly push similar sentences closer and dissimilar sentences farther apart.\n",
    "\n",
    "### The Key Difference: Fine-tuning for Similarity\n",
    "\n",
    "The critical takeaway is that SBERT **fine-tunes** BERT (or RoBERTa, XLNet, etc.) specifically for the task of generating *semantically meaningful sentence embeddings for similarity comparisons*.\n",
    "\n",
    "During this fine-tuning, the weights of the BERT model are adjusted such that the pooling output (the sentence embedding) now accurately reflects semantic similarity. The model learns to map similar sentences to nearby points in the vector space.\n",
    "\n",
    "### Why this is a Game Changer:\n",
    "\n",
    "1.  **Superior Embeddings:** The embeddings generated by SBERT (after fine-tuning) are significantly better for semantic similarity tasks than those from vanilla BERT.\n",
    "2.  **Efficiency:** Once SBERT is fine-tuned, you can pre-compute and store the embeddings for all sentences in your database. When a new query comes in, you pass it *once* through the SBERT model to get its embedding, and then use highly efficient vector similarity search (like cosine similarity) to find the most relevant sentences in your pre-computed database. This is orders of magnitude faster than passing every candidate through BERT for each query.\n",
    "3.  **Versatility:** SBERT models can be used out-of-the-box for various tasks like semantic search, clustering, and paraphrase detection without further task-specific fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "In essence, SBERT takes a powerful general-purpose language model (BERT) and adapts it through a clever training architecture (siamese/triplet networks) to become an equally powerful, and highly efficient, tool for generating high-quality **sentence embeddings specifically optimized for semantic similarity**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a1de4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c5cb990",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Indexing and Search\n",
    "\n",
    "Following the generation of embeddings (covered previously), the next critical steps in Vector Search are **indexing** and **searching** these vector spaces efficiently. This involves addressing two primary challenges:\n",
    "\n",
    "1.  How to measure the distance (similarity) between vectors.\n",
    "2.  How to search vectors in a fast and scalable way.\n",
    "\n",
    "## 1\\. Measuring Vector Distance (Similarity Metrics)\n",
    "\n",
    ">> Note: (some of) these are implemented in `sklearn.metrics.pairwise`.\n",
    "\n",
    "In a multi-dimensional vector space, we need quantitative methods to determine how \"close\" or \"similar\" two vectors are. The choice of metric depends on the embedding model and specific use case.\n",
    "\n",
    "Here are five widely used metrics:\n",
    "\n",
    "  * **L0 \"Norm\" (Hamming Distance for binary vectors, or count of non-zero elements):**\n",
    "      * **Definition:** For a vector, it is the number of non-zero elements. For comparing two vectors, it can be conceptualized as the number of positions at which the corresponding elements are different (often related to Hamming distance for binary vectors).\n",
    "      * **Interpretation:** Primarily measures **sparsity** or **difference in presence/absence** of features.\n",
    "      * **Note:** While commonly referred to as the L0 \"norm,\" it technically doesn't satisfy all the properties of a mathematical norm (specifically, homogeneity: `||cx|| = |c| ||x||` does not hold for `c=0` or when `x` has non-zero elements). It's more accurately described as a **pseudo-norm** or a count. It is **not commonly used for continuous vector similarity in dense embeddings** (like those from neural networks) but is highly relevant in fields like sparse coding, feature selection, and comparing binary/categorical data.\n",
    "  * **Manhattan Distance (L1 Distance):**\n",
    "      * **Definition:** Sum of the absolute differences between the corresponding coordinates of two points. `L1(x, y) = Σ |xi - yi|`\n",
    "      * **Interpretation:** Measures distance in a grid-like pattern, moving only along axes.\n",
    "  * **Euclidean Distance (L2 Distance / Squared Distance):**\n",
    "      * **Definition:** Square root of the sum of the squared differences between the corresponding coordinates of two points. `L2(x, y) = √(Σ (xi - yi)²)`\n",
    "      * **Interpretation:** Measures the shortest straight-line distance between two points.\n",
    "  * **Cosine Distance (Cosine Similarity):**\n",
    "      * **Definition:** Measures the angle between two vectors. Cosine similarity is 1 for identical directions (angle 0) and 0 for orthogonal directions (angle 90 degrees). Cosine *distance* is often `1 - Cosine Similarity`.\n",
    "      * **Interpretation:** Focuses on the similarity of **direction**, regardless of magnitude.\n",
    "      * **Note:** this is the dot product of two versions.\n",
    "  * **Dot Product Distance (Inner Product Distance):**\n",
    "      * **Definition:** Based on the projection of one vector onto another (sum of the products of corresponding components). `DotProduct(x, y) = Σ (xi * yi)`\n",
    "      * **Interpretation:** Considers similarity in terms of both **direction and magnitude**.\n",
    "\n",
    "\n",
    "### 1.1 Visualising\n",
    "\n",
    "You can use Principal Component Analysis (PCA). \n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "PCA_model = PCA(n_components=2) # project in 2D space\n",
    "PCA_model.fit(matrix_of_embedding_vectors)\n",
    "PCA_model.transform(embedding_vector)\n",
    "```\n",
    "\n",
    "\n",
    "## 2\\. Fast and Scalable Vector Search\n",
    "\n",
    "Once distances can be measured, the challenge shifts to finding similar vectors efficiently within potentially vast vector spaces (millions to billions of embeddings).\n",
    "\n",
    "### Search Algorithms:\n",
    "\n",
    "1.  **Brute Force Algorithm:**\n",
    "\n",
    "      * **Process:**\n",
    "          1.  Calculate distances from the query vector to *all* other vectors in the space.\n",
    "          2.  Sort all distances.\n",
    "          3.  Find the top `k` nearest vectors.\n",
    "      * **Complexity:** `O(N*d)` where `N` is the number of vectors and `d` is the number of dimensions.\n",
    "      * **Drawback:** Impractical and computationally bottlenecked for large datasets (`N` in the millions/billions).\n",
    "\n",
    "2.  **Approximate Nearest Neighbor (ANN) Algorithms:**\n",
    "\n",
    "      * **Concept:** Accelerate search by trading a small amount of accuracy for significant speed improvements. They avoid exhaustive search by intelligently pruning the search space.\n",
    "      * **How it works (general idea):** Divides the search space into smaller partitions, indexes them using data structures (like trees or hashes), and then searches only the most relevant partitions.\n",
    "      * **Example (TreeAh - shallow tree and asymmetric hashing):** A production-ready algorithm that uses tree structures for indexing.\n",
    "\n",
    "### ScaNN: Scalable Approximate Nearest Neighbor\n",
    "\n",
    "In 2020, Google Research introduced **ScaNN** (Scalable Approximate Nearest Neighbor), a leading ANN algorithm powering services like Google Search and YouTube's recommendation system. \n",
    "\n",
    "ScaNN achieves fast and scalable vector search by combining:\n",
    "\n",
    "1.  **Reduced Search Space (Space Pruning):**\n",
    "      * **Multilevel Tree Search:** The vector space is divided into hierarchical partitions. A search tree represents this structure, with nodes as centroids of partitions.\n",
    "      * **Pruning:** During a query, the tree is traversed (from root to branches to leaves), and irrelevant partitions are pruned, focusing the search on the most relevant sub-partitions.\n",
    "2.  **Compressed Vector Size (Data Quantization):**\n",
    "      * **Technique:** Compresses data points to save space and reduce indexing time (e.g., reducing a 9-dimensional vector from 9 floats to 12 bits).\n",
    "3.  **Increased Ranking Efficiency (Business Logic Integration):**\n",
    "      * **Filtering:** Incorporates business logic to filter results based on specific criteria (e.g., \"resorts in the United States,\" \"red dresses\") *before* or *during* similarity ranking, restricting the search to a relevant subset of the dataset.\n",
    "\n",
    "## Vertex AI Vector Search (formerly Matching Engine)\n",
    "\n",
    "Vertex AI Vector Search is a fully managed similarity vector search service provided by Google Cloud.\n",
    "\n",
    "  * **Foundation:** Utilizes an advanced version of the ScaNN algorithm.\n",
    "  * **Benefits:** Offers fast searching, low latencies, and scalability to billions of vectors, often at a lower cost compared to similar services.\n",
    "\n",
    "-----\n",
    "\n",
    "## Further Reading & Resources\n",
    "\n",
    "### Approximate Nearest Neighbor (ANN) Algorithms & Vector Databases:\n",
    "\n",
    "  * **ScaNN: Efficient Vector Similarity Search (Google AI Blog):**\n",
    "      * The official announcement and explanation of ScaNN.\n",
    "      * [Link: ScaNN: Efficient Vector Similarity Search](https://www.google.com/search?q=https://ai.googleblog.com/2020/07/scann-efficient-vector-similarity-search.html)\n",
    "  * **The Missing Piece: An Introduction to Approximate Nearest Neighbor (ANN) Search (Pinecone Blog):**\n",
    "      * A good overview of ANN concepts.\n",
    "      * [Link: Introduction to ANN Search](https://www.google.com/search?q=https://www.pinecone.io/learn/approximate-nearest-neighbor/)\n",
    "  * **Faiss (Facebook AI Similarity Search):**\n",
    "      * An open-source library for efficient similarity search. Essential for understanding ANN implementations.\n",
    "      * [GitHub: Faiss](https://github.com/facebookresearch/faiss)\n",
    "  * **HNSW (Hierarchical Navigable Small Worlds):**\n",
    "      * A popular graph-based ANN algorithm widely used in vector databases.\n",
    "      * [Paper: Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs](https://arxiv.org/abs/1603.09320)\n",
    "  * **Vertex AI Vector Search Documentation:**\n",
    "      * Official Google Cloud documentation for their managed vector search service.\n",
    "      * [Link: Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/matching-engine/overview) (Look for the most current link for \"Vector Search\" if \"Matching Engine\" is outdated)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75472c8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5eec2ed",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF is a numerical statistic reflecting how important a word is to a document in a collection or corpus. It's widely used in information retrieval and text mining.\n",
    "\n",
    "It increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "### Core Components:\n",
    "\n",
    "TF-IDF is calculated by multiplying two main components:\n",
    "\n",
    "1.  **Term Frequency (TF):** Measures how frequently a term (word) appears in a document. There are several ways to define TF:\n",
    "      * **Raw Count:** The absolute count of the term in the document. `TF(t, d) = count(t in d)`\n",
    "      * **Normalized Frequency:** Raw count divided by the total number of terms in the document. This prevents bias towards longer documents. `TF(t, d) = count(t in d) / total_terms_in_d`\n",
    "      * **Log Normalization:** `TF(t, d) = 1 + log(count(t in d))` (if count > 0)\n",
    "      * **Interpretation:** Higher TF means the term is more relevant to that specific document.\n",
    "\n",
    "2.  **Inverse Document Frequency (IDF):** Measures how important a term is across the entire corpus. It's designed to down-weight common words (like \"the,\" \"is,\" \"a\") that appear in many documents and up-weight rare words that are more distinctive.\n",
    "      * **Definition:** `IDF(t, D) = log(N / df(t))`\n",
    "          * `N`: Total number of documents in the corpus.\n",
    "          * `df(t)`: Number of documents in the corpus that contain the term `t` (document frequency).\n",
    "          * `log`: Usually base 10 or natural log (`ln`). A `+1` is often added to the denominator to prevent division by zero if a term doesn't appear in any document.\n",
    "      * **Interpretation:** Higher IDF means the term is rarer across the corpus, making it potentially more informative.\n",
    "\n",
    "### TF-IDF Calculation:\n",
    "\n",
    "The final TF-IDF score for a term `t` in a document `d` within a corpus `D` is:\n",
    "\n",
    "`TF-IDF(t, d, D) = TF(t, d) * IDF(t, D)`\n",
    "\n",
    "#### How it Works:\n",
    "\n",
    "  * If a term appears frequently in a document (high TF) but rarely across the entire corpus (high IDF), it gets a **high TF-IDF score**, indicating it's very relevant to that specific document.\n",
    "  * If a term appears frequently across the entire corpus (low IDF), even if it's frequent in a document (high TF), its TF-IDF score will be lower, reducing its perceived importance.\n",
    "  * If a term is rare everywhere (low TF and high IDF), its score can still be significant if it appears in the document.\n",
    "\n",
    "### Output Representation:\n",
    "\n",
    "  * When applied to a document, TF-IDF transforms the document into a **sparse vector**. Each dimension of the vector corresponds to a unique term in the vocabulary, and its value is the TF-IDF score for that term in that document.\n",
    "  * Since most words in the vocabulary will not appear in a given document, most values in the vector will be zero, hence \"sparse.\"\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "  * **Information Retrieval:** Ranking documents by relevance to a query.\n",
    "  * **Text Summarization:** Identifying key terms in a document.\n",
    "  * **Document Classification:** As features for machine learning models.\n",
    "  * **Keyword Extraction:** Finding the most important keywords in a text.\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "  * **Semantic Blindness:** TF-IDF treats each word as independent. It doesn't understand synonyms, polysemy, or the semantic relationship between words.\n",
    "  * **Lack of Context:** It only considers word presence and frequency, not the word order or surrounding context within a sentence.\n",
    "  * **Vocabulary Size:** Performance can degrade with very large vocabularies, leading to extremely high-dimensional, sparse vectors.\n",
    "\n",
    "### Further Reading:\n",
    "\n",
    "  * **TF-IDF (Wikipedia):** The standard reference for the algorithm.\n",
    "      * [Link: Tf-idf - Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "  * **Scikit-learn TfidfVectorizer:** Practical implementation details and usage.\n",
    "      * [Link: sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "  * **Introduction to Information Retrieval (Manning, Raghavan, Schütze):** Chapter 6 covers TF-IDF in depth.\n",
    "      * [Link: Introduction to Information Retrieval - Chapter 6](https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
